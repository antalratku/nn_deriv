{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from time import perf_counter_ns\n",
    "import copy\n",
    "import pickle\n",
    "import gc\n",
    "from simple_network import SimpleNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the options_case flag to True uses input parameter counts which correspond to the parameter count in the following options pricing models:\n",
    "\n",
    "* 8: Heston model with no dividend yield: $P_{8} = \\{\\kappa, \\theta, \\sigma, \\rho, v_{0}, K, \\tau, r \\}$;\n",
    "* 9: Heston model with continuous dividend yield: $P_{9} = P_{8} \\cup \\{ q \\}$;\n",
    "* 11: Bates model: $P_{9} \\cup \\{\\lambda_{v}, \\mu_{v}\\}$;\n",
    "* 14: Duffie model with correlated jumps only (SVJJ): $P_{14} = P_{9} \\cup \\{\\lambda_{c}, \\mu_{cv}, \\mu_{cy}, s_{cy}, \\rho_{j} \\}$;\n",
    "* 19: Full Duffie model: $P_{19} = P_{14} \\cup \\{\\lambda_{y}, \\mu_{y}, s_{y}, \\lambda_{v}, \\mu_{v} \\}$.\n",
    "\n",
    "If the options_case flag is set to False, a more generic performance analysis is performed for 16, 32 and 128 input parameters, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval = 10000\n",
    "options_case = True\n",
    "device = torch.device('cuda') # alternatively torch.device('cpu')\n",
    "\n",
    "if options_case:\n",
    "    params_in = [8, 9, 11, 14, 19]\n",
    "    suffix = 'options'\n",
    "else:\n",
    "    params_in = [16, 32, 128]\n",
    "    suffix = 'generic'\n",
    "\n",
    "layer_sizes_jac = [\n",
    "    # [32, 32, 32, 32, 1],\n",
    "    # [64, 64, 64, 64, 1],\n",
    "    # [128, 128, 128, 128, 1],\n",
    "    # [256, 256, 256, 256, 1],\n",
    "    [32, 32, 32, 32, 4],\n",
    "    [64, 64, 64, 64, 4],\n",
    "    [128, 128, 128, 128, 4],\n",
    "    [256, 256, 256, 256, 4],\n",
    "    [32, 32, 32, 32, 16],\n",
    "    [64, 64, 64, 64, 16],\n",
    "    [128, 128, 128, 128, 16],\n",
    "    [256, 256, 256, 256, 16]\n",
    "]\n",
    "layer_sizes_hess = [\n",
    "    [32, 32, 1],\n",
    "    [64, 64, 1],\n",
    "    [128, 128, 1],\n",
    "    [256, 256, 1]\n",
    "]\n",
    "    \n",
    "act_funs = [nn.Sigmoid, nn.Tanh, nn.Sigmoid, nn.Tanh, nn.Sigmoid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the Jacobian calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_jacobian_calculations(num_eval, params_in, layer_sizes, act_funs, calc_deriv, device):\n",
    "    jacobians = torch.zeros(size=(num_eval, layer_sizes[-1], params_in), device=device)\n",
    "    times = torch.zeros(size=(num_eval,))\n",
    "    cuda_runs = 1\n",
    "    for seed_idx in range(num_eval):\n",
    "        torch.manual_seed(seed_idx)\n",
    "        x = torch.rand(params_in, device=device).float().requires_grad_(not calc_deriv)\n",
    "        model = SimpleNetwork(params_in, copy.deepcopy(layer_sizes), act_funs, calc_deriv, device).to(device)\n",
    "        if not calc_deriv:\n",
    "            if device == torch.device('cuda'):\n",
    "                jac = torch.autograd.functional.jacobian(model, x) # warmup\n",
    "                torch.cuda.synchronize()\n",
    "                start = perf_counter_ns()\n",
    "                for _ in range(cuda_runs):\n",
    "                    _ = torch.autograd.functional.jacobian(model, x)\n",
    "                torch.cuda.synchronize()\n",
    "                end = perf_counter_ns()\n",
    "                times[seed_idx] = (end - start) / cuda_runs\n",
    "            else:\n",
    "                start = perf_counter_ns()\n",
    "                jac = torch.autograd.functional.jacobian(model, x)\n",
    "                end = perf_counter_ns()\n",
    "                times[seed_idx] = end - start\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if device == torch.device('cuda'):\n",
    "                    jac = model.get_network_jacobian(x) # warmup\n",
    "                    torch.cuda.synchronize()\n",
    "                    start = perf_counter_ns()\n",
    "                    for c_idx in range(cuda_runs):\n",
    "                        _ = model.get_network_jacobian(x + (c_idx + 1)*1e-10) # s.t. network derivatives are recalculated\n",
    "                    torch.cuda.synchronize()\n",
    "                    end = perf_counter_ns()\n",
    "                    times[seed_idx] = (end - start) / cuda_runs\n",
    "                else:\n",
    "                    start = perf_counter_ns()\n",
    "                    jac = model.get_network_jacobian(x)\n",
    "                    end = perf_counter_ns()\n",
    "                    times[seed_idx] = end - start\n",
    "            model.destroy_model()\n",
    "        jacobians[seed_idx, :, :] = jac.detach().clone()\n",
    "        del model, jac, x\n",
    "        if device == torch.device('cuda'):\n",
    "            torch.cuda.synchronize()\n",
    "    return (jacobians, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_idx, p in enumerate(params_in):\n",
    "    for l_idx, l in enumerate(layer_sizes_jac):\n",
    "        gc.disable()\n",
    "        print(f'Running Jacobian calculations with {p} params in, and layer sizes {l}')\n",
    "        jacobians_pt, t_jac_pt = run_jacobian_calculations(num_eval, p, l, act_funs[0:len(l)], False, device=device)\n",
    "        jacobians_mat, t_jac_mat = run_jacobian_calculations(num_eval, p, l, act_funs[0:len(l)], True, device=device)\n",
    "        jac_abs_diff_max = torch.max(torch.abs(jacobians_mat - jacobians_pt))\n",
    "        assert jac_abs_diff_max < 1e-7\n",
    "\n",
    "        print(f'Average time - Jacobian - Pytorch : {torch.mean(t_jac_pt)} nanoseconds')\n",
    "        print(f'Average time - Jacobian - Matrix : {torch.mean(t_jac_mat)} nanoseconds')\n",
    "        print(f'Median time - Jacobian - Pytorch : {torch.median(t_jac_pt)} nanoseconds')\n",
    "        print(f'Median time - Jacobian - Matrix : {torch.median(t_jac_mat)} nanoseconds')\n",
    "        \n",
    "        times_diff_jac = {\n",
    "            'times_pt': t_jac_pt.detach().cpu().numpy(),\n",
    "            'times_mat': t_jac_mat.detach().cpu().numpy(),\n",
    "            'max_diff': jac_abs_diff_max.detach().cpu().numpy()\n",
    "        }\n",
    "        with open(f'rev_times_{suffix}_{p}_{l[0]}_{l[-1]}_{device.type}.p', 'wb') as file:\n",
    "            pickle.dump(times_diff_jac, file)\n",
    "\n",
    "        gc.enable()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the Hessian calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hessian_calculations(num_eval, params_in, layer_sizes, act_funs, calc_deriv, device):\n",
    "    hessians = torch.zeros(size=(num_eval, params_in, layer_sizes[-1], params_in), device=device)\n",
    "    times = torch.zeros(size=(num_eval,))\n",
    "    cuda_runs = 1\n",
    "    for seed_idx in range(num_eval):\n",
    "        torch.manual_seed(seed_idx)\n",
    "        x = torch.rand(params_in, device=device).float().requires_grad_(not calc_deriv)\n",
    "        model = SimpleNetwork(params_in, copy.deepcopy(layer_sizes), act_funs, calc_deriv, device).to(device)\n",
    "        if not calc_deriv:\n",
    "            if device == torch.device('cuda'):\n",
    "                hess = torch.autograd.functional.hessian(model, x) # warmup\n",
    "                torch.cuda.synchronize()\n",
    "                start = perf_counter_ns()\n",
    "                for _ in range(cuda_runs):\n",
    "                    _ = torch.autograd.functional.hessian(model, x)\n",
    "                end = perf_counter_ns()\n",
    "                times[seed_idx] = (end - start) / cuda_runs\n",
    "            else:\n",
    "                start = perf_counter_ns()\n",
    "                hess = torch.autograd.functional.hessian(model, x)\n",
    "                end = perf_counter_ns()\n",
    "                times[seed_idx] = end - start\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if device == torch.device('cuda'):\n",
    "                    hess = model.get_network_hessians(x) # warmup\n",
    "                    torch.cuda.synchronize()\n",
    "                    start = perf_counter_ns()\n",
    "                    for c_idx in range(cuda_runs):\n",
    "                        _ = model.get_network_hessians(x + (c_idx + 1)*1e-10) # s.t. network derivatives are recalculated\n",
    "                    torch.cuda.synchronize()\n",
    "                    end = perf_counter_ns()\n",
    "                    times[seed_idx] = (end - start) / cuda_runs\n",
    "                else:\n",
    "                    start = perf_counter_ns()\n",
    "                    hess = model.get_network_hessians(x)\n",
    "                    end = perf_counter_ns()\n",
    "                    times[seed_idx] = end - start\n",
    "            model.destroy_model()\n",
    "        hessians[seed_idx, :, :, :] = hess.view(params_in, layer_sizes[-1], params_in).detach().clone()\n",
    "        del model, hess, x\n",
    "        if device == torch.device('cuda'):\n",
    "            torch.cuda.synchronize()\n",
    "    return (hessians, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_idx, p in enumerate(params_in):\n",
    "    for l_idx, l in enumerate(layer_sizes_hess):\n",
    "        gc.disable()\n",
    "        print(f'Running Hessian calculations with {p} params in, and layer sizes {l}')\n",
    "        hessians_pt, t_hess_pt = run_hessian_calculations(num_eval, p, l, act_funs[0:len(l)], False, device=device)\n",
    "        hessians_mat, t_hess_mat = run_hessian_calculations(num_eval, p, l, act_funs[0:len(l)], True, device=device)\n",
    "        hess_abs_diff_max = torch.max(torch.abs(hessians_mat - hessians_pt))\n",
    "        assert hess_abs_diff_max < 1e-7\n",
    "        \n",
    "        print(f'Average time - Hessian - Pytorch : {torch.mean(t_hess_pt)} nanoseconds')\n",
    "        print(f'Average time - Hessian - Matrix : {torch.mean(t_hess_mat)} nanoseconds')\n",
    "        print(f'Median time - Hessian - Pytorch : {torch.median(t_hess_pt)} nanoseconds')\n",
    "        print(f'Median time - Hessian - Matrix : {torch.median(t_hess_mat)} nanoseconds')\n",
    "\n",
    "        times_diff_hess = {\n",
    "            'times_pt': t_hess_pt.detach().cpu().numpy(),\n",
    "            'times_mat': t_hess_mat.detach().cpu().numpy(),\n",
    "            'max_diff': hess_abs_diff_max.detach().cpu().numpy()\n",
    "        }\n",
    "        with open(f'rev_times_{suffix}_hess_{p}_{l[0]}_{l[-1]}_{device.type}.p', 'wb') as file:\n",
    "            pickle.dump(times_diff_hess, file)\n",
    "\n",
    "        gc.enable()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b788eaf2147201216768cf8bcd729d0dfd6c216ff51a0c05d2023b44dd9a9a86"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('nn_deriv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
