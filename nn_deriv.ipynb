{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from time import time_ns\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from hashlib import sha256"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self, params_in: int, layer_sizes: list(), act_funs: list(), calc_deriv: bool):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "        \n",
    "        self.in_out = layer_sizes\n",
    "        self.in_out.insert(0, params_in)\n",
    "        self.ins = self.in_out[0:-1]\n",
    "        self.outs = self.in_out[1:]\n",
    "        self.act_funs = act_funs\n",
    "        self.layer_count = len(self.act_funs)\n",
    "        self.calc_deriv = calc_deriv\n",
    "        \n",
    "        assert len(self.ins) == len(self.act_funs)\n",
    "        assert len(self.outs) == len(self.act_funs)\n",
    "        assert all([f in [nn.Sigmoid, nn.Tanh] for f in self.act_funs])\n",
    "        \n",
    "        for layer_idx, (n_in, n_out, act_fun) in enumerate(zip(self.ins, self.outs, self.act_funs), 1):\n",
    "            layer = nn.Linear(n_in, n_out, bias=True)\n",
    "            self.add_module(f'fc{layer_idx}', layer)\n",
    "            self.add_module(f'act{layer_idx}', self.act_funs[layer_idx-1]())\n",
    "\n",
    "        self.requires_grad_(not calc_deriv)\n",
    "        if calc_deriv:\n",
    "            self.fwd_activation = {}\n",
    "            def get_fwd_activation(name):\n",
    "                def hook(model, input, output):\n",
    "                    self.fwd_activation[name] = output.detach()\n",
    "                return hook\n",
    "            for name, module in self.named_modules():\n",
    "                if name != '':\n",
    "                    _ = module.register_forward_hook(get_fwd_activation(name))\n",
    "        self.x_hash = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.ins[0])\n",
    "        if self.calc_deriv and self._recalculate_derivatives(x):\n",
    "            self.layer_derivatives_1 = dict()\n",
    "            self.layer_derivatives_2 = dict()\n",
    "        for layer_idx in range(1, self.layer_count + 1):\n",
    "            x = self._modules[f'fc{layer_idx}'](x)\n",
    "            x = self._modules[f'act{layer_idx}'](x)\n",
    "        return x\n",
    "    \n",
    "    def _recalculate_derivatives(self, x):\n",
    "        new_hash = sha256(np.ascontiguousarray(x.detach().numpy()))\n",
    "        if (not self.x_hash is None) and (self.x_hash.digest() == new_hash.digest()):\n",
    "            return False\n",
    "        else:\n",
    "            self.x_hash = new_hash\n",
    "            return True\n",
    "    \n",
    "    def _get_analytic_derivative_1(self, actfun, x):\n",
    "        if type(actfun) == nn.Sigmoid:\n",
    "            return actfun(x) * (1 - actfun(x))\n",
    "        elif type(actfun) == nn.Tanh:\n",
    "            return 1 - actfun(x)**2\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def _get_analytic_derivative_2(self, actfun, x):\n",
    "        if type(actfun) == nn.Sigmoid:\n",
    "            return actfun(x) * (1 - actfun(x)) * (1 - 2*actfun(x))\n",
    "        elif type(actfun) == nn.Tanh:\n",
    "            return -2*actfun(x)*(1 - actfun(x)**2)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def _calculate_layer_derivatives_1(self):\n",
    "        for layer_idx in range(1, self.layer_count + 1):\n",
    "            layer_weights = self._modules[f'fc{layer_idx}'].weight\n",
    "            act_d1 = self._modules[f'act{layer_idx}']\n",
    "            act_input = self.fwd_activation[f'fc{layer_idx}']\n",
    "            layer_act = self._get_analytic_derivative_1(act_d1, act_input)\n",
    "            self.layer_derivatives_1.update(\n",
    "                {layer_idx: layer_act.view(layer_act.shape[0], layer_act.shape[1], 1) * layer_weights})\n",
    "    \n",
    "    def _calculate_subnetwork_jacobian(self, p, q, x):\n",
    "        if (len(self.layer_derivatives_1) == 0) or self._recalculate_derivatives(x):\n",
    "            self._calculate_layer_derivatives_1()\n",
    "        if ((p == 1) and (q == 0)):\n",
    "            return torch.eye(self.ins[0]).repeat(self.layer_derivatives_1[1].shape[0], 1, 1)\n",
    "        if ((p == self.layer_count + 1) and (q == self.layer_count)):\n",
    "            return torch.eye(self.outs[-1]).repeat(self.layer_derivatives_1[1].shape[0], 1, 1)\n",
    "        else:\n",
    "            if not ((p >= 1) and (p <= self.layer_count) and (q >= p) and (q <= self.layer_count)):\n",
    "                raise ValueError()\n",
    "            subjac = self.layer_derivatives_1[q]\n",
    "            for l in range(self.layer_count - q + 1, self.layer_count - p + 1):\n",
    "                subjac = torch.matmul(subjac, self.layer_derivatives_1[self.layer_count - l])\n",
    "            return subjac\n",
    "    \n",
    "    def get_network_jacobian(self, x):\n",
    "        x = x.view(-1, self.ins[0])\n",
    "        _ = self.forward(x)\n",
    "        return self._calculate_subnetwork_jacobian(1, self.layer_count, x)\n",
    "\n",
    "    def get_network_hessian_slice(self, x, j):\n",
    "        x = x.view(-1, self.ins[0])\n",
    "        _ = self.forward(x)\n",
    "        hessian_slice = torch.zeros(x.shape[0], self.outs[-1], self.ins[0])\n",
    "        for l in range(1, self.layer_count + 1):\n",
    "            phi_pre = self._calculate_subnetwork_jacobian(1, l-1, x)\n",
    "            phi_post = self._calculate_subnetwork_jacobian(l+1, self.layer_count, x)\n",
    "            layer_weights = self._modules[f'fc{l}'].weight\n",
    "            act_d2 = self._modules[f'act{l}']\n",
    "            act_input = self.fwd_activation[f'fc{l}']\n",
    "            layer_act = self._get_analytic_derivative_2(act_d2, act_input)\n",
    "            m = torch.matmul(layer_weights, phi_pre)\n",
    "            phi = (layer_act.unsqueeze(2) * m[:, :, j].unsqueeze(2)) * layer_weights\n",
    "            hessian_slice += torch.matmul(torch.matmul(phi_post, phi), phi_pre)\n",
    "        return hessian_slice\n",
    "    \n",
    "    def get_network_hessians(self, x):\n",
    "        x = x.view(-1, self.ins[0])\n",
    "        _ = self.forward(x)\n",
    "        hessians = torch.zeros(x.shape[0], self.ins[0], self.outs[-1], self.ins[0])\n",
    "        for l in range(1, self.layer_count + 1):\n",
    "            phi_pre = self._calculate_subnetwork_jacobian(1, l-1, x)\n",
    "            phi_post = self._calculate_subnetwork_jacobian(l+1, self.layer_count, x)\n",
    "            layer_weights = self._modules[f'fc{l}'].weight\n",
    "            act_d2 = self._modules[f'act{l}']\n",
    "            act_input = self.fwd_activation[f'fc{l}']\n",
    "            layer_act = self._get_analytic_derivative_2(act_d2, act_input)\n",
    "            m = torch.matmul(layer_weights, phi_pre)\n",
    "            phi = (layer_act.unsqueeze(2) * m).transpose(1, 2).unsqueeze(3) * layer_weights\n",
    "            hessians += torch.matmul(torch.matmul(phi_post.unsqueeze(1), phi), phi_pre.unsqueeze(1))\n",
    "        return hessians"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting the options_case flag to True uses input parameter counts which correspond to the parameter count in the following options pricing models:\n",
    "\n",
    "* 8: Heston model with no dividend yield: $P_{8} = \\{\\kappa, \\theta, \\sigma, \\rho, v_{0}, K, \\tau, r \\}$;\n",
    "* 9: Heston model with continuous dividend yield: $P_{9} = P_{8} \\cup \\{ q \\}$;\n",
    "* 11: Bates model: $P_{9} \\cup \\{\\lambda_{v}, \\mu_{v}\\}$;\n",
    "* 14: Duffie model with correlated jumps only (SVJJ): $P_{14} = P_{9} \\cup \\{\\lambda_{c}, \\mu_{cv}, \\mu_{cy}, s_{cy}, \\rho_{j} \\}$;\n",
    "* 19: Full Duffie model: $P_{19} = P_{14} \\cup \\{\\lambda_{y}, \\mu_{y}, s_{y}, \\lambda_{v}, \\mu_{v} \\}$.\n",
    "\n",
    "If the options_case flag is set to False, a more generic performance analysis is performed for 16, 32 and 128 input parameters, respectively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_eval = 10000\n",
    "options_case = True\n",
    "\n",
    "if options_case:\n",
    "    params_in = [8, 9, 11, 14, 19]\n",
    "else:    \n",
    "    params_in = [16, 32, 128]\n",
    "\n",
    "layer_sizes_jac = [\n",
    "    [32, 32, 32, 32, 1],\n",
    "    [64, 64, 64, 64, 1],\n",
    "    [128, 128, 128, 128, 1],\n",
    "    [256, 256, 256, 256, 1],\n",
    "    [32, 32, 32, 32, 4],\n",
    "    [64, 64, 64, 64, 4],\n",
    "    [128, 128, 128, 128, 4],\n",
    "    [256, 256, 256, 256, 4],\n",
    "    [32, 32, 32, 32, 16],\n",
    "    [64, 64, 64, 64, 16],\n",
    "    [128, 128, 128, 128, 16],\n",
    "    [256, 256, 256, 256, 16]\n",
    "]\n",
    "layer_sizes_hess = [\n",
    "    [32, 32, 1],\n",
    "    [64, 64, 1],\n",
    "    [128, 128, 1],\n",
    "    [256, 256, 1]\n",
    "]\n",
    "    \n",
    "act_funs = [nn.Sigmoid, nn.Tanh, nn.Sigmoid, nn.Tanh, nn.Sigmoid]\n",
    "\n",
    "times_jac_pt = np.zeros(shape=(num_eval, len(params_in), len(layer_sizes_jac)))\n",
    "times_jac_mat = np.zeros(shape=(num_eval, len(params_in), len(layer_sizes_jac)))\n",
    "jac_abs_diff_max = 0\n",
    "\n",
    "times_hess_pt = np.zeros(shape=(num_eval, len(params_in), len(layer_sizes_hess)))\n",
    "times_hess_mat = np.zeros(shape=(num_eval, len(params_in), len(layer_sizes_hess)))\n",
    "hess_abs_diff_max = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison of the Jacobian calculations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_jacobian_calculations(num_eval, params_in, layer_sizes, act_funs, calc_deriv):\n",
    "    jacobians = []\n",
    "    times = []\n",
    "    for seed_idx in range(num_eval):\n",
    "        torch.manual_seed(seed_idx)\n",
    "        x = torch.rand(params_in).float().requires_grad_(not calc_deriv)\n",
    "        model = SimpleNetwork(params_in, copy.deepcopy(layer_sizes), act_funs, calc_deriv)\n",
    "        start = time_ns()\n",
    "        if not calc_deriv:\n",
    "            jac = torch.autograd.functional.jacobian(model, x)\n",
    "        else:\n",
    "            jac = model.get_network_jacobian(x)\n",
    "        end = time_ns()\n",
    "        times.append(end - start)\n",
    "        jacobians.append(jac)\n",
    "    return (jacobians, times)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for p in range(len(params_in)):\n",
    "    for l in range(len(layer_sizes_jac)):\n",
    "        print(f'Running Jacobian calculations with {params_in[p]} params in, and layer sizes {layer_sizes_jac[l]}')\n",
    "        jacobians_pt, t_jac_pt = \\\n",
    "            run_jacobian_calculations(num_eval, params_in[p],\n",
    "                                      layer_sizes_jac[l], act_funs[0:len(layer_sizes_jac[l])], False)\n",
    "        jacobians_mat, t_jac_mat = \\\n",
    "            run_jacobian_calculations(num_eval, params_in[p],\n",
    "                                      layer_sizes_jac[l], act_funs[0:len(layer_sizes_jac[l])], True)\n",
    "        for i in range(num_eval):\n",
    "            jac_abs_diff_iter = (jacobians_mat[i].squeeze() - jacobians_pt[i]).abs().max()\n",
    "            assert jac_abs_diff_iter < 1e-7\n",
    "            if (jac_abs_diff_iter > jac_abs_diff_max):\n",
    "                jac_abs_diff_max = jac_abs_diff_iter\n",
    "            \n",
    "        print(f'Average time - Jacobian - Pytorch : {np.mean(t_jac_pt)} nanoseconds')\n",
    "        print(f'Average time - Jacobian - Matrix : {np.mean(t_jac_mat)} nanoseconds')\n",
    "        print(f'Median time - Jacobian - Pytorch : {np.median(t_jac_pt)} nanoseconds')\n",
    "        print(f'Median time - Jacobian - Matrix : {np.median(t_jac_mat)} nanoseconds')\n",
    "        times_jac_pt[:, p, l] = t_jac_pt\n",
    "        times_jac_mat[:, p, l] = t_jac_mat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison of the Hessian calculations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_hessian_calculations(num_eval, params_in, layer_sizes, act_funs, calc_deriv):\n",
    "    hessians = []\n",
    "    times = []\n",
    "    for seed_idx in range(num_eval):\n",
    "        torch.manual_seed(seed_idx)\n",
    "        x = torch.rand(params_in).float().requires_grad_(not calc_deriv)\n",
    "        model = SimpleNetwork(params_in, copy.deepcopy(layer_sizes), act_funs, calc_deriv)\n",
    "        start = time_ns()\n",
    "        if not calc_deriv:\n",
    "            hess = torch.autograd.functional.hessian(model, x)\n",
    "        else:\n",
    "            hess = model.get_network_hessians(x)\n",
    "        end = time_ns()\n",
    "        times.append(end - start)\n",
    "        hessians.append(hess)\n",
    "    return (hessians, times)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for p in range(len(params_in)):\n",
    "    for l in range(len(layer_sizes_hess)):\n",
    "        print(f'Running Hessian calculations with {params_in[p]} params in, and layer sizes {layer_sizes_hess[l]}')\n",
    "        hessians_pt, t_hess_pt = \\\n",
    "            run_hessian_calculations(num_eval, params_in[p],\n",
    "                                     layer_sizes_hess[l], act_funs[0:len(layer_sizes_hess[l])], False)\n",
    "        hessians_mat, t_hess_mat = \\\n",
    "            run_hessian_calculations(num_eval, params_in[p],\n",
    "                                     layer_sizes_hess[l], act_funs[0:len(layer_sizes_hess[l])], True)\n",
    "        for i in range(num_eval):\n",
    "            hess_abs_diff_iter = (hessians_mat[i].squeeze() - hessians_pt[i]).abs().max()\n",
    "            assert hess_abs_diff_iter < 1e-7\n",
    "            if (hess_abs_diff_iter > hess_abs_diff_max):\n",
    "                hess_abs_diff_max = hess_abs_diff_iter\n",
    "        print(f'Average time - Hessian - Pytorch : {np.mean(t_hess_pt)} nanoseconds')\n",
    "        print(f'Average time - Hessian - Matrix : {np.mean(t_hess_mat)} nanoseconds')\n",
    "        print(f'Median time - Hessian - Pytorch : {np.median(t_hess_pt)} nanoseconds')\n",
    "        print(f'Median time - Hessian - Matrix : {np.median(t_hess_mat)} nanoseconds')\n",
    "        times_hess_pt[:, p, l] = t_hess_pt\n",
    "        times_hess_mat[:, p, l] = t_hess_mat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if options_case:\n",
    "    suffix = 'options'\n",
    "else:\n",
    "    suffix = 'generic'\n",
    "\n",
    "pickle.dump(times_jac_pt, open(f'rev_times_{suffix}_jac_pt.p', 'wb'))\n",
    "pickle.dump(times_jac_mat, open(f'rev_times_{suffix}_jac_mat.p', 'wb'))\n",
    "\n",
    "pickle.dump(times_hess_pt, open(f'rev_times_{suffix}_hess_pt.p', 'wb'))\n",
    "pickle.dump(times_hess_mat, open(f'rev_times_{suffix}_hess_mat.p', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nn_deriv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "ca54c40b131d648793346062a9de6d87e9518f7c7567f7cb111f66af57748e3a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}