{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from time import perf_counter_ns\n",
    "import copy\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self, params_in: int, layer_sizes: list(), act_funs: list(), calc_deriv: bool, device: torch.device):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "        \n",
    "        self.in_out = layer_sizes\n",
    "        self.in_out.insert(0, params_in)\n",
    "        self.ins = self.in_out[0:-1]\n",
    "        self.outs = self.in_out[1:]\n",
    "        self.act_funs = act_funs\n",
    "        self.layer_count = len(self.act_funs)\n",
    "        self.calc_deriv = calc_deriv\n",
    "        self.device = device\n",
    "        \n",
    "        assert len(self.ins) == len(self.act_funs)\n",
    "        assert len(self.outs) == len(self.act_funs)\n",
    "        assert all([f in [nn.Sigmoid, nn.Tanh] for f in self.act_funs])\n",
    "        \n",
    "        for layer_idx, (n_in, n_out, act_fun) in enumerate(zip(self.ins, self.outs, self.act_funs), 1):\n",
    "            layer = nn.Linear(n_in, n_out, bias=True)\n",
    "            self.add_module(f'fc{layer_idx}', layer)\n",
    "            self.add_module(f'act{layer_idx}', self.act_funs[layer_idx-1]())\n",
    "\n",
    "        self.requires_grad_(not calc_deriv)\n",
    "        if calc_deriv:\n",
    "            self.fwd_activation = {}\n",
    "            def get_fwd_activation(name):\n",
    "                def hook(model, input, output):\n",
    "                    self.fwd_activation[name] = output.detach()\n",
    "                return hook\n",
    "            for name, module in self.named_modules():\n",
    "                if name != '':\n",
    "                    _ = module.register_forward_hook(get_fwd_activation(name))\n",
    "        self.last_x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.ins[0])\n",
    "        if self.calc_deriv and self._recalculate_derivatives(x):\n",
    "            self.layer_derivatives_1 = dict()\n",
    "            self.layer_derivatives_2 = dict()\n",
    "        for layer_idx in range(1, self.layer_count + 1):\n",
    "            x = self._modules[f'fc{layer_idx}'](x)\n",
    "            x = self._modules[f'act{layer_idx}'](x)\n",
    "        return x\n",
    "    \n",
    "    def _recalculate_derivatives(self, x):\n",
    "        if (self.last_x is not None) and (self.last_x.shape == x.shape) and (torch.all(torch.eq(self.last_x, x))):\n",
    "            return False\n",
    "        self.last_x = x\n",
    "        return True\n",
    "    \n",
    "    def _get_analytic_derivative_1(self, actfun, x):\n",
    "        if type(actfun) == nn.Sigmoid:\n",
    "            return actfun(x) * (1 - actfun(x))\n",
    "        elif type(actfun) == nn.Tanh:\n",
    "            return 1 - actfun(x)**2\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def _get_analytic_derivative_2(self, actfun, x):\n",
    "        if type(actfun) == nn.Sigmoid:\n",
    "            return actfun(x) * (1 - actfun(x)) * (1 - 2*actfun(x))\n",
    "        elif type(actfun) == nn.Tanh:\n",
    "            return -2*actfun(x)*(1 - actfun(x)**2)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def _calculate_layer_derivatives_1(self):\n",
    "        for layer_idx in range(1, self.layer_count + 1):\n",
    "            layer_weights = self._modules[f'fc{layer_idx}'].weight\n",
    "            act_d1 = self._modules[f'act{layer_idx}']\n",
    "            act_input = self.fwd_activation[f'fc{layer_idx}']\n",
    "            layer_act = self._get_analytic_derivative_1(act_d1, act_input)\n",
    "            self.layer_derivatives_1.update(\n",
    "                {layer_idx: layer_act.view(layer_act.shape[0], layer_act.shape[1], 1) * layer_weights})\n",
    "    \n",
    "    def _calculate_subnetwork_jacobian(self, p, q, x):\n",
    "        if (len(self.layer_derivatives_1) == 0) or self._recalculate_derivatives(x):\n",
    "            self._calculate_layer_derivatives_1()\n",
    "        if ((p == 1) and (q == 0)):\n",
    "            return torch.eye(self.ins[0], device=self.device).repeat(self.layer_derivatives_1[1].shape[0], 1, 1)\n",
    "        if ((p == self.layer_count + 1) and (q == self.layer_count)):\n",
    "            return torch.eye(self.outs[-1], device=self.device).repeat(self.layer_derivatives_1[1].shape[0], 1, 1)\n",
    "        else:\n",
    "            if not ((p >= 1) and (p <= self.layer_count) and (q >= p) and (q <= self.layer_count)):\n",
    "                raise ValueError()\n",
    "            subjac = self.layer_derivatives_1[q]\n",
    "            for l in range(self.layer_count - q + 1, self.layer_count - p + 1):\n",
    "                subjac = torch.matmul(subjac, self.layer_derivatives_1[self.layer_count - l])\n",
    "            return subjac\n",
    "    \n",
    "    def get_network_jacobian(self, x):\n",
    "        x = x.view(-1, self.ins[0])\n",
    "        _ = self.forward(x)\n",
    "        return self._calculate_subnetwork_jacobian(1, self.layer_count, x)\n",
    "\n",
    "    def get_network_hessian_slice(self, x, j):\n",
    "        x = x.view(-1, self.ins[0])\n",
    "        _ = self.forward(x)\n",
    "        hessian_slice = torch.zeros(x.shape[0], self.outs[-1], self.ins[0])\n",
    "        for l in range(1, self.layer_count + 1):\n",
    "            phi_pre = self._calculate_subnetwork_jacobian(1, l-1, x)\n",
    "            phi_post = self._calculate_subnetwork_jacobian(l+1, self.layer_count, x)\n",
    "            layer_weights = self._modules[f'fc{l}'].weight\n",
    "            act_d2 = self._modules[f'act{l}']\n",
    "            act_input = self.fwd_activation[f'fc{l}']\n",
    "            layer_act = self._get_analytic_derivative_2(act_d2, act_input)\n",
    "            m = torch.matmul(layer_weights, phi_pre)\n",
    "            phi = (layer_act.unsqueeze(2) * m[:, :, j].unsqueeze(2)) * layer_weights\n",
    "            hessian_slice += torch.matmul(torch.matmul(phi_post, phi), phi_pre)\n",
    "        return hessian_slice\n",
    "    \n",
    "    def get_network_hessians(self, x):\n",
    "        x = x.view(-1, self.ins[0])\n",
    "        _ = self.forward(x)\n",
    "        hessians = torch.zeros(x.shape[0], self.ins[0], self.outs[-1], self.ins[0], device=self.device)\n",
    "        for l in range(1, self.layer_count + 1):\n",
    "            phi_pre = self._calculate_subnetwork_jacobian(1, l-1, x)\n",
    "            phi_post = self._calculate_subnetwork_jacobian(l+1, self.layer_count, x)\n",
    "            layer_weights = self._modules[f'fc{l}'].weight\n",
    "            act_d2 = self._modules[f'act{l}']\n",
    "            act_input = self.fwd_activation[f'fc{l}']\n",
    "            layer_act = self._get_analytic_derivative_2(act_d2, act_input)\n",
    "            m = torch.matmul(layer_weights, phi_pre)\n",
    "            phi = (layer_act.unsqueeze(2) * m).transpose(1, 2).unsqueeze(3) * layer_weights\n",
    "            hessians += torch.matmul(torch.matmul(phi_post.unsqueeze(1), phi), phi_pre.unsqueeze(1))\n",
    "        return hessians\n",
    "    \n",
    "    def destroy_model(self):\n",
    "        del self.layer_derivatives_1, self.layer_derivatives_2, self.fwd_activation, self._modules\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the options_case flag to True uses input parameter counts which correspond to the parameter count in the following options pricing models:\n",
    "\n",
    "* 8: Heston model with no dividend yield: $P_{8} = \\{\\kappa, \\theta, \\sigma, \\rho, v_{0}, K, \\tau, r \\}$;\n",
    "* 9: Heston model with continuous dividend yield: $P_{9} = P_{8} \\cup \\{ q \\}$;\n",
    "* 11: Bates model: $P_{9} \\cup \\{\\lambda_{v}, \\mu_{v}\\}$;\n",
    "* 14: Duffie model with correlated jumps only (SVJJ): $P_{14} = P_{9} \\cup \\{\\lambda_{c}, \\mu_{cv}, \\mu_{cy}, s_{cy}, \\rho_{j} \\}$;\n",
    "* 19: Full Duffie model: $P_{19} = P_{14} \\cup \\{\\lambda_{y}, \\mu_{y}, s_{y}, \\lambda_{v}, \\mu_{v} \\}$.\n",
    "\n",
    "If the options_case flag is set to False, a more generic performance analysis is performed for 16, 32 and 128 input parameters, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval = 10000\n",
    "options_case = True\n",
    "device = torch.device('cuda') # alternatively torch.device('cpu')\n",
    "\n",
    "if options_case:\n",
    "    params_in = [8, 9, 11, 14, 19]\n",
    "    suffix = 'options'\n",
    "else:\n",
    "    params_in = [16, 32, 128]\n",
    "    suffix = 'generic'\n",
    "\n",
    "layer_sizes_jac = [\n",
    "    # [32, 32, 32, 32, 1],\n",
    "    # [64, 64, 64, 64, 1],\n",
    "    # [128, 128, 128, 128, 1],\n",
    "    # [256, 256, 256, 256, 1],\n",
    "    [32, 32, 32, 32, 4],\n",
    "    [64, 64, 64, 64, 4],\n",
    "    [128, 128, 128, 128, 4],\n",
    "    [256, 256, 256, 256, 4],\n",
    "    [32, 32, 32, 32, 16],\n",
    "    [64, 64, 64, 64, 16],\n",
    "    [128, 128, 128, 128, 16],\n",
    "    [256, 256, 256, 256, 16]\n",
    "]\n",
    "layer_sizes_hess = [\n",
    "    [32, 32, 1],\n",
    "    [64, 64, 1],\n",
    "    [128, 128, 1],\n",
    "    [256, 256, 1]\n",
    "]\n",
    "    \n",
    "act_funs = [nn.Sigmoid, nn.Tanh, nn.Sigmoid, nn.Tanh, nn.Sigmoid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the Jacobian calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_jacobian_calculations(num_eval, params_in, layer_sizes, act_funs, calc_deriv, device):\n",
    "    jacobians = torch.zeros(size=(num_eval, layer_sizes[-1], params_in), device=device)\n",
    "    times = torch.zeros(size=(num_eval,))\n",
    "    cuda_runs = 1\n",
    "    for seed_idx in range(num_eval):\n",
    "        torch.manual_seed(seed_idx)\n",
    "        x = torch.rand(params_in, device=device).float().requires_grad_(not calc_deriv)\n",
    "        model = SimpleNetwork(params_in, copy.deepcopy(layer_sizes), act_funs, calc_deriv, device).to(device)\n",
    "        if not calc_deriv:\n",
    "            if device == torch.device('cuda'):\n",
    "                jac = torch.autograd.functional.jacobian(model, x) # warmup\n",
    "                torch.cuda.synchronize()\n",
    "                start = perf_counter_ns()\n",
    "                for _ in range(cuda_runs):\n",
    "                    _ = torch.autograd.functional.jacobian(model, x)\n",
    "                torch.cuda.synchronize()\n",
    "                end = perf_counter_ns()\n",
    "                times[seed_idx] = (end - start) / cuda_runs\n",
    "            else:\n",
    "                start = perf_counter_ns()\n",
    "                jac = torch.autograd.functional.jacobian(model, x)\n",
    "                end = perf_counter_ns()\n",
    "                times[seed_idx] = end - start\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if device == torch.device('cuda'):\n",
    "                    jac = model.get_network_jacobian(x) # warmup\n",
    "                    torch.cuda.synchronize()\n",
    "                    start = perf_counter_ns()\n",
    "                    for c_idx in range(cuda_runs):\n",
    "                        _ = model.get_network_jacobian(x + (c_idx + 1)*1e-10) # s.t. network derivatives are recalculated\n",
    "                    torch.cuda.synchronize()\n",
    "                    end = perf_counter_ns()\n",
    "                    times[seed_idx] = (end - start) / cuda_runs\n",
    "                else:\n",
    "                    start = perf_counter_ns()\n",
    "                    jac = model.get_network_jacobian(x)\n",
    "                    end = perf_counter_ns()\n",
    "                    times[seed_idx] = end - start\n",
    "            model.destroy_model()\n",
    "        jacobians[seed_idx, :, :] = jac.detach().clone()\n",
    "        del model, jac, x\n",
    "        if device == torch.device('cuda'):\n",
    "            torch.cuda.synchronize()\n",
    "    return (jacobians, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_idx, p in enumerate(params_in):\n",
    "    for l_idx, l in enumerate(layer_sizes_jac):\n",
    "        gc.disable()\n",
    "        print(f'Running Jacobian calculations with {p} params in, and layer sizes {l}')\n",
    "        jacobians_pt, t_jac_pt = run_jacobian_calculations(num_eval, p, l, act_funs[0:len(l)], False, device=device)\n",
    "        jacobians_mat, t_jac_mat = run_jacobian_calculations(num_eval, p, l, act_funs[0:len(l)], True, device=device)\n",
    "        jac_abs_diff_max = torch.max(torch.abs(jacobians_mat - jacobians_pt))\n",
    "        assert jac_abs_diff_max < 1e-7\n",
    "\n",
    "        print(f'Average time - Jacobian - Pytorch : {torch.mean(t_jac_pt)} nanoseconds')\n",
    "        print(f'Average time - Jacobian - Matrix : {torch.mean(t_jac_mat)} nanoseconds')\n",
    "        print(f'Median time - Jacobian - Pytorch : {torch.median(t_jac_pt)} nanoseconds')\n",
    "        print(f'Median time - Jacobian - Matrix : {torch.median(t_jac_mat)} nanoseconds')\n",
    "        \n",
    "        times_diff_jac = {\n",
    "            'times_pt': t_jac_pt.detach().cpu().numpy(),\n",
    "            'times_mat': t_jac_mat.detach().cpu().numpy(),\n",
    "            'max_diff': jac_abs_diff_max.detach().cpu().numpy()\n",
    "        }\n",
    "        with open(f'rev_times_{suffix}_{p}_{l[0]}_{l[-1]}_{device.type}.p', 'wb') as file:\n",
    "            pickle.dump(times_diff_jac, file)\n",
    "\n",
    "        gc.enable()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the Hessian calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hessian_calculations(num_eval, params_in, layer_sizes, act_funs, calc_deriv, device):\n",
    "    hessians = torch.zeros(size=(num_eval, params_in, layer_sizes[-1], params_in), device=device)\n",
    "    times = torch.zeros(size=(num_eval,))\n",
    "    cuda_runs = 1\n",
    "    for seed_idx in range(num_eval):\n",
    "        torch.manual_seed(seed_idx)\n",
    "        x = torch.rand(params_in, device=device).float().requires_grad_(not calc_deriv)\n",
    "        model = SimpleNetwork(params_in, copy.deepcopy(layer_sizes), act_funs, calc_deriv, device).to(device)\n",
    "        if not calc_deriv:\n",
    "            if device == torch.device('cuda'):\n",
    "                hess = torch.autograd.functional.hessian(model, x) # warmup\n",
    "                torch.cuda.synchronize()\n",
    "                start = perf_counter_ns()\n",
    "                for _ in range(cuda_runs):\n",
    "                    _ = torch.autograd.functional.hessian(model, x)\n",
    "                end = perf_counter_ns()\n",
    "                times[seed_idx] = (end - start) / cuda_runs\n",
    "            else:\n",
    "                start = perf_counter_ns()\n",
    "                hess = torch.autograd.functional.hessian(model, x)\n",
    "                end = perf_counter_ns()\n",
    "                times[seed_idx] = end - start\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if device == torch.device('cuda'):\n",
    "                    hess = model.get_network_hessians(x) # warmup\n",
    "                    torch.cuda.synchronize()\n",
    "                    start = perf_counter_ns()\n",
    "                    for c_idx in range(cuda_runs):\n",
    "                        _ = model.get_network_hessians(x + (c_idx + 1)*1e-10) # s.t. network derivatives are recalculated\n",
    "                    torch.cuda.synchronize()\n",
    "                    end = perf_counter_ns()\n",
    "                    times[seed_idx] = (end - start) / cuda_runs\n",
    "                else:\n",
    "                    start = perf_counter_ns()\n",
    "                    hess = model.get_network_hessians(x)\n",
    "                    end = perf_counter_ns()\n",
    "                    times[seed_idx] = end - start\n",
    "            model.destroy_model()\n",
    "        hessians[seed_idx, :, :, :] = hess.view(params_in, layer_sizes[-1], params_in).detach().clone()\n",
    "        del model, hess, x\n",
    "        if device == torch.device('cuda'):\n",
    "            torch.cuda.synchronize()\n",
    "    return (hessians, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_idx, p in enumerate(params_in):\n",
    "    for l_idx, l in enumerate(layer_sizes_hess):\n",
    "        gc.disable()\n",
    "        print(f'Running Hessian calculations with {p} params in, and layer sizes {l}')\n",
    "        hessians_pt, t_hess_pt = run_hessian_calculations(num_eval, p, l, act_funs[0:len(l)], False, device=device)\n",
    "        hessians_mat, t_hess_mat = run_hessian_calculations(num_eval, p, l, act_funs[0:len(l)], True, device=device)\n",
    "        hess_abs_diff_max = torch.max(torch.abs(hessians_mat - hessians_pt))\n",
    "        assert hess_abs_diff_max < 1e-7\n",
    "        \n",
    "        print(f'Average time - Hessian - Pytorch : {torch.mean(t_hess_pt)} nanoseconds')\n",
    "        print(f'Average time - Hessian - Matrix : {torch.mean(t_hess_mat)} nanoseconds')\n",
    "        print(f'Median time - Hessian - Pytorch : {torch.median(t_hess_pt)} nanoseconds')\n",
    "        print(f'Median time - Hessian - Matrix : {torch.median(t_hess_mat)} nanoseconds')\n",
    "\n",
    "        times_diff_hess = {\n",
    "            'times_pt': t_hess_pt.detach().cpu().numpy(),\n",
    "            'times_mat': t_hess_mat.detach().cpu().numpy(),\n",
    "            'max_diff': hess_abs_diff_max.detach().cpu().numpy()\n",
    "        }\n",
    "        with open(f'rev_times_{suffix}_hess_{p}_{l[0]}_{l[-1]}_{device.type}.p', 'wb') as file:\n",
    "            pickle.dump(times_diff_hess, file)\n",
    "\n",
    "        gc.enable()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b788eaf2147201216768cf8bcd729d0dfd6c216ff51a0c05d2023b44dd9a9a86"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('nn_deriv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
